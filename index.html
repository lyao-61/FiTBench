<h1>FiTBench:Benchmark for Scene Graph Anticipation Leveraging Fine-graind Semantic Cues</h1>
<h2>Supplementary Material</h2>
<h1>Datasets Statistic</h1>
<h2>Object and Relationship Statistic</h2>
<p>For FISC-AG dataset, we provide annotations for 234, 253 frames with a total of 476, 229 bounding boxes of 35 object classes (excluding 'person'), and 1, 715, 568 instances of 25 relationship classes. For FISC-PVSG dataset, we provide annotations for 149, 484 frames with a total of 216, 855 bounding boxes of 128 object classes, and 117, 964 instances of 72 relationship classes. The following two figures visualizes the distribution of object and relationship categories in FISC-AG and FISC-PVSG, respectively. As shown in these two figure, compared with the FISC-AG dataset, the FISC-VidOR dataset has more object and relationship categories, increasing the data diversity. Some objects (e.g., 'floor' and 'food') and relations (e.g., 'holding' and 'in front of') occur frequently, while others (e.g., 'cookie' and 'squeezing') occur only a few times. However, even with such a distribution, for FISC-AG dataset, almost all objects have at least 10K instances and every relationship as at least 1K instances. For FISC-PVSG dataset, almost all objects have at least 1.5K instances and every relationship as at least 1.5K instances.
<img src="https://github.com/lyao-61/FiTBench/blob/main/supplementary_material/fig_object.png?raw=true" alt="Distribution of objects."></p>
<p>Distribution of objects in (a) FISC-AG dataset and (b) FISC-PVSG dataset. We divide objects into 12 types based on their attributes and usage scenarios. Each type is shown in a different color. The AG dataset includes 8 object types, while the PVSG dataset includes 12.
<img src="https://github.com/lyao-61/FiTBench/supplementary_material/fig_relations.png" alt="在这里插入图片描述"></p>
<p>Distribution of relationships in (a) FISC-AG dataset and (b) FISC-PVSG dataset. The FISC-pvsg dataset contains a wider range of relationships than the FISC-AG dataset.</p>
<h2>Emotion Annotation Statistic</h2>
<p>We annotated 53 emotion categories in the FISC-AG dataset and 54 in the FISC-PVSG dataset. Emotions often reflect intentions or behaviors. For example, 'focus' might suggest that a person is using or focusing on an object, while 'surprise' might suggest sudden attention. Emotions, physical states, and how objects are used can also affect relationships. Positive emotions may show liking or use of an object, while negative emotions may suggest dislike or avoidance. 'Tired' may change how a character uses an object, and 'pain' might require specific ones (e.g., 'medicine'). We also handled unclear cases carefully to avoid errors while keeping the dataset reliable. Based on these considerations, we grouped the emotion labels into seven categories:</p>
<ul>
<li><strong>Neutral Emotions</strong>: An objective state without significant emotional tendencies, reflected in a baseline mental orientation that is calm (e.g., 'relaxed') and without significant emotional fluctuations (e.g., 'neutral').</li>
<li><strong>Positive Emotions</strong>: Reflects positive emotional experiences, including psychologically motivating emotional expressions such as 'joyful' and 'content' etc.</li>
<li><strong>Concentration &amp; Thinking</strong>: Refers to a mental state where attention is strongly directed toward a specific task or goal (e.g., "concentrated", "thoughtful").</li>
<li><strong>Surprise &amp; Confusion</strong>: Labels transient emotional responses triggered by sudden stimulation, covering a spectrum of intensity from curiosity (e.g., 'curious') to surprised (e.g., 'astonished').</li>
<li><strong>Negative Emotions</strong>: Characterizes a collection of negative emotions, including 'sadness', 'anger', and other emotional patterns that have a tendency to be psychologically inhibited.</li>
<li><strong>Physiological Emotions</strong>: Perceived states of direct feedback from the body such as 'sleepy', 'hungry', and other manifestations of the biological functions of the associated organism.</li>
<li><strong>Miscellaneous</strong>: Labeling uncertainty in mood determination due to incomplete information or ambiguity in expression.
<img src="https://github.com/lyao-61/FiTBench/supplementary_material/fig_emotion_distribution.png" alt="Distribution of emotions.">
Distribution of emotions in (a) FISC-AG dataset and (b) FISC-PVSG dataset. Different types of emotions are marked with different colors.
The different types of emotions are distributed evenly, with positive emotions being the most common and neutral emotions less frequent. However, emotions within the same category are not evenly distributed. For example, in both datasets, 'happy' is the most frequent positive emotion. This may be because most of the video data comes from everyday life scenes. While expressions that require more in-depth analysis, such as 'engaged', may be less frequently captured or annotated.</li>
</ul>
<h2>State Annotation Statistic</h2>
<p>We labeled the FISC-AG dataset and the FISC-PVSG dataset with 115 and 124 categories of state labels, respectively.
We argue that physical attributes influence interactions: basic attributes directly impact how a character interacts with an object. States like food condition or container status provide domain-specific information that helps the model reason more accurately in certain scenarios (e.g., a kitchen). Spatial location also shapes the logic behind a character’s actions. To handle ambiguity, we allow uncertain states when needed and define clear states to prevent the model from making incorrect assumptions.
Therefore, we distinguish status labels by the following 9 categories:</p>
<ul>
<li><strong>Physical state</strong>: describes the current condition, position, or form of an object, encompassing attributes, such as 'opened', 'broken' etc.</li>
<li><strong>Functional state</strong> is used to describe the real-time operational state of a system, device, or object, covering whether it is in a specific functional mode such as 'active', 'idle', 'active' etc.</li>
<li><strong>Cleanliness &amp; Maintenance</strong> used to assess the state of hygiene and maintenance of a place or object, covering different levels of status from 'dirty', 'disorganized', and 'damaged' to 'clean', 'washed', or 'polished'.</li>
<li><strong>Unsage &amp; Consumption</strong> describes the various states of items in relation to their preparation, condition, or depletion, such as 'empty', 'half-full', etc.</li>
<li><strong>Spatial State</strong> describes the position, direction, motion state and interaction with the environment of an object in three-dimensional space, covering specific spatial attributes such as 'floating', 'attached', 'upright', 'in-hand', 'seated' or 'covered'.</li>
<li><strong>Visual Property</strong> describes the visual characteristics of an object or scene, covering a wide range of properties such as material (e.g., 'wooden', 'tiled'), color (e.g., 'green', 'dark'), surface condition (e.g., 'paved', 'smooth', 'striped'), environmental features (e.g., 'lush'), and visual effects (e.g., 'blurred'), which are used to accurately convey its appearance.</li>
<li><strong>Dynamic State</strong> describes an object or system in a constantly changing, active state, such as 'moving', 'rolling', 'shaky' etc.</li>
<li><strong>Biological &amp; Environmental</strong> factors dynamically interact to influence states ranging from physiological needs (e.g., 'hungry', 'injured') and emotional responses (e.g., 'happy', 'scared') to environmental adaptations (e.g., 'wilting', 'wet').</li>
<li><strong>Uncertainty state</strong> is a special state characterization used to describe the situation when there is missing or incomplete information in the system, reflecting the uncertain knowledge and incomplete mastery of the current situation or data reliability.
<img src="https://github.com/lyao-61/FiTBench/supplementary_material/fig_state_distribution.png" alt="Distribution of states.">
Distribution of states in (a) FISC-AG dataset and (b) FISC-PVSG dataset. Different types of states are marked with different colors.
The types of states are evenly distributed overall. In the FISC-AG dataset, states related to usage and consumption are more common, while in the FISC-PVSG dataset, states related to biological and environmental aspects appear most frequently. This may be because FISC-AG mainly contains indoor scenes, whereas FISC-PVSG includes more complex scenes.
Some states (e.g., 'open', 'dirty') occur frequently, while others (e.g., 'boiled') occur only a few times. This is because clean or open environments may be more prevalent, while broken or dead are relatively rare. These high-frequency categories are more common in the real world and are more likely to be captured during the data collection process.</li>
</ul>
<h1>Pipeline</h1>
<p>In order to capture the dynamic semantic cues in each frame, we built an automatic annotation process based on the large-scale visual language model Qwen2.5VL-7B. The process is shown in figure.
<img src="https://github.com/lyao-61/FiTBench/supplementary_material/fig_pipeline.png" alt="pipeline">
Part of (a) is used to obtain frame-level video caption. Qwen2.5VL-7B has powerful image description capabilities, and we input frame-by-frame video into VLM. Most of the results obtained are compliant video subtitles, and very few placeholders (e.g., 'addCriterion') are output. We manually removed the abnormal samples. We obtained video captions for 298,384 video frames on a single Nvidia-3090, which took about 90h and resulted in valid video captions for 262,454 video frames.
Part of (b) is used to obtain emotion annotation and state annotation. We take each frame of image input and restrict its output to word semantic labeling with these two prompt: "Describe the expression of the {person} in the picture with one word. Don't output anything other than the emotion. Your output should be in the following format: 'emotion'." and "Describe the state of the {object} in the picture with one word. Don't output anything other than the state. Your output should be in the following format: 'state'." We manually removed output placeholders and noise labels (e.g., incomplete words such as “str”), merged near-synonyms (e.g., 'concentrateded' to 'concentrated'), and adjusted the word properties of a small number of words (e.g., 'sleeping' to 'sleepy'). For FISC-AG dataset, we obtained emotion annotations for 2983384 video frames on a single Nvidia-3090, which took about 22h, and eventually obtained valid expression annotations for 287585 video frames of subject. It takes about 45h to obtain state annotations, and finally obtain valid state annotations for 264141 video frames. For FISC-PVSG dataset, we obtained 46,236 valid emotion annotations for 69966 exo-centric video frames, and 117,445 valid states for 149,484 video frames.
For obtaining character bounding boxes and object bounding boxes, we extract all involved subject-object pairs based on the existing &lt; subject-relationship-object &gt; triples to ensure that the bounding box annotations are consistent with the predicted goals of the scene graph. The subject-object pairs of interest are entered into Qwen2.5VL-7B along with the video frames, and Qwen2.5VL-7B can effectively localize the bounding boxes of the objects we are interested in. As for the few invalid placeholders and noise bounding boxes are removed manually. Finally 139655 subject bounding boxes and 216855 object bounding boxes are obtained for the FISC-PVSG dataset.</p>
<h1>Experiments Results</h1>
<p>$\mathcal{F}$ denotes the overall proportion of initial video segments input to the model, which serves to assess the model's predictive ability over different time spans by controlling the length of video content observed by the model. Specifically, the value of $\mathcal{F}$ (e.g., 0.3, 0.5, 0.7, 0.9) represents the fact that the model can access only the first 30%, 50%, 70%, or 90% of the video as input and needs to predict the subsequent unobserved portions based on this. By adjusting the size of $\mathcal{F}$, the researcher is able to analyze the model's ability to capture and reason about short-term relationships (e.g., when $\mathcal{F}$ is large, the model relies on longer known segments for proximity prediction) and long-term relationships (e.g., when $\mathcal{F}$ is small, the model needs to infer potential correlations farther into the future from limited information), thus comprehensively validating the model's adaptability and generalization in different time-series scenarios.
To better understand the role of dynamic semantic semantic annotations such as subject emotion, object state, and caption, we evaluate multiple scene graph anticipation baselines on our proposed FISCBench, which consists of two augmented datasets: FISC-AG and FISC-PVSG. We added the Text-Augmented Visual Semantic (TAVS) module to the six baselines and measured the relative performance changes over the two datasets. The results reveal different trends across models.</p>
<ul>
<li>In the FISC-AG dataset, the model basically shows a boosting trend. At $\mathcal{F}$=0.9, the DSGdetr+ w/TAVS method boosts substantially. It is possible that when visual features extracted by ORPU (Object Representation Processing Unit) and SCPU (Spatial Context Processing Unit) are aligned with textual descriptions at the semantic level, the textual module can improve the performance by enhancing the object localization and relational reasoning. In particular, when the model is equipped with a dual-decoding approach (e.g., STTran++ w/TAVS and DSGDetr++ w/TAVS.), significant improvements of 37.36% and 45.64% are obtained. On the SceneSayerODE w/TAVS method there is also a boost (e.g., at $\mathcal{F}$=0.9, the average boost is 0.24%), but it is not as effective as the other methods.</li>
<li>In the FISC-PVSG dataset, most of the models performed well, with the DSGdetr++ w/TAVS method improving by 50.99% at $\mathcal{F}$=0.9. It performs poorly in the DSGdetr+ w/TAVS approach. Since DSGdetr++ has an additional temporal encoder, it may be more effective in handling text and visual fusion, whereas DSGdetr+ may lack such a structure, leading to insufficient information integration. The design of the loss function may also affect the results. For example, DSGdetr++ decoding both observed and predicted relationships may improve performance in multimodal data, whereas models that only decode predictions may not be able to fully utilize the textual information, resulting in limited or decreased improvement. In addition, different models have different sensitivities to text features, and some may overfit text noise.</li>
</ul>
<h1>Qualitative Experiment</h1>
<p>We show some of the results of the SceneSayerSDE method and the SceneSayerSDE w/TAVS method on long-term future prediction and short-term future prediction in figure. Short-term predictions usually rely on immediate contextual information, such as the current action or the position of an object. Textual features such as emotion, state, or caption may provide additional semantic information to help the model more accurately capture the relationships in the current scene. For example, a text describing “handing a cup” can assist the model in recognizing the intent of the action and thus inferring the next action in short-term prediction, e.g., “picking up the cup”. Long-term prediction requires an understanding of more complex temporal relationships and underlying intentions. Text features may contain higher-level information about the whole scene or character's goals. In this case, text features provide long-term structural cues for the model to predict events further in the future. In this case, text features provide long-term structured cues for the model to predict events further into the future.
<img src="https://github.com/lyao-61/FiTBench/supplementary_material/fig_visualize_supp.png" alt="visualize_supp"></p>
